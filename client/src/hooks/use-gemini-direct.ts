import { useState, useCallback, useRef, useEffect } from 'react';
import { useToast } from '@/hooks/use-toast';

interface Message {
  id: string;
  userId: number;
  message: string;
  response?: string;
  timestamp: string;
  emotion?: string;
  personality?: any;
  audioData?: ArrayBuffer;
  hasAudio?: boolean;
}

export function useGeminiDirect(userId: number) {
  const [messages, setMessages] = useState<Message[]>([]);
  const [isConnected, setIsConnected] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [currentEmotion, setCurrentEmotion] = useState<string>('happy');
  const [logs, setLogs] = useState<string[]>([]);
  const websocketRef = useRef<WebSocket | null>(null);
  const audioChunksRef = useRef<ArrayBuffer[]>([]);
  const { toast } = useToast();

  // EXACT configuration from working debug component
  const GEMINI_API_KEY = import.meta.env.VITE_GEMINI_API_KEY;
  const wsUrl = `wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1alpha.GenerativeService.BidiGenerateContent?key=${GEMINI_API_KEY}`;

  // EXACT setup message that works for audio
  const setupMessage = {
    setup: {
      model: 'models/gemini-2.0-flash-live-001',
      generationConfig: {
        responseModalities: ['AUDIO']
      },
      systemInstruction: {
        parts: [{
          text: `Voc√™ √© Lele, uma IA companheira de 7 anos que √© muito amig√°vel, curiosa e brincalhona. 
          Voc√™ fala portugu√™s brasileiro de forma natural para uma crian√ßa de 7 anos - usando palavras simples, 
          sendo muito entusiasmada e ocasionalmente usando emojis.
          
          Suas respostas devem ser:
          - Curtas (1-2 frases)
          - Entusiasmadas e positivas
          - Adequadas para crian√ßas
          - Em portugu√™s brasileiro
          - Com personalidade de uma crian√ßa de 7 anos
          - Use voz feminina jovem e amig√°vel`
        }]
      }
    }
  };

  const addLog = useCallback((message: string) => {
    const timestamp = new Date().toLocaleTimeString();
    setLogs(prev => [...prev, `[${timestamp}] ${message}`]);
    console.log(`[Gemini Direct] ${message}`);
  }, []);

  // EXACT audio playback logic from debug component
  const playAudioBuffer = useCallback(async (arrayBuffer: ArrayBuffer) => {
    try {
      addLog(`üéµ Playing audio data: ${arrayBuffer.byteLength} bytes`);
      
      // EXACT method from debug component
      const audioContext = new AudioContext();
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(audioContext.destination);
      
      return new Promise<void>((resolve) => {
        source.onended = () => {
          addLog('üîä Audio playback completed');
          resolve();
        };
        source.start();
        addLog('üîä Audio playback started');
      });
    } catch (error) {
      console.error('Audio playback error:', error);
      addLog(`‚ùå Audio playback failed: ${error}`);
      throw error;
    }
  }, [addLog]);

  // Function to concatenate and play collected audio chunks
  const playCollectedAudio = useCallback(async () => {
    if (audioChunksRef.current.length === 0) {
      addLog('‚ùå No audio chunks to play');
      return;
    }

    try {
      // Calculate total size
      const totalSize = audioChunksRef.current.reduce((sum, chunk) => sum + chunk.byteLength, 0);
      addLog(`üéµ Concatenating ${audioChunksRef.current.length} audio chunks, total: ${totalSize} bytes`);

      // Concatenate all chunks
      const concatenatedBuffer = new ArrayBuffer(totalSize);
      const concatenatedArray = new Uint8Array(concatenatedBuffer);
      let offset = 0;

      for (const chunk of audioChunksRef.current) {
        concatenatedArray.set(new Uint8Array(chunk), offset);
        offset += chunk.byteLength;
      }

      // Play the complete audio
      await playAudioBuffer(concatenatedBuffer);

      // Update the last message with complete audio data
      setMessages(prev => {
        const updated = [...prev];
        if (updated.length > 0) {
          const lastMessage = updated[updated.length - 1];
          lastMessage.audioData = concatenatedBuffer;
          lastMessage.hasAudio = true;
        }
        return updated;
      });

      // Clear chunks for next response
      audioChunksRef.current = [];
      
    } catch (error) {
      addLog(`‚ùå Failed to play concatenated audio: ${error}`);
      audioChunksRef.current = []; // Clear on error
    }
  }, [addLog, playAudioBuffer]);

  // EXACT message handling logic from debug component
  const handleMessage = useCallback(async (event: MessageEvent) => {
    if (event.data instanceof Blob) {
      addLog(`üì® Received Blob: ${event.data.size} bytes, type: ${event.data.type}`);
      
      // Try to read as text first (for JSON responses)
      try {
        const text = await event.data.text();
        addLog(`üìù Blob text content preview: ${text.substring(0, 100)}...`);
        
        // Try to parse as JSON
        try {
          const jsonData = JSON.parse(text);
          addLog(`‚úÖ JSON parsed successfully`);
          
          // Check for server content and response types
          if (jsonData.serverContent) {
            addLog('‚úÖ Server content received');
            const { modelTurn } = jsonData.serverContent;
            if (modelTurn && modelTurn.parts) {
              const textPart = modelTurn.parts.find((part: any) => part.text);
              const audioPart = modelTurn.parts.find((part: any) => part.inlineData);
              
              if (textPart) {
                addLog(`üí¨ TEXT response: ${textPart.text.substring(0, 50)}...`);
              }
              if (audioPart) {
                addLog(`üîä AUDIO response detected! MIME: ${audioPart.inlineData.mimeType}`);
                // Handle audio data if present
              }
            }
          }
          
          if (jsonData.setupComplete) {
            addLog('‚úÖ Setup complete - ready for audio responses');
          }
          
          // Check for error messages
          if (jsonData.error) {
            addLog(`‚ùå Error: ${jsonData.error.message || jsonData.error}`);
          }
        } catch (e) {
          addLog('‚ùå Not valid JSON - might be audio data');
        }
      } catch (e) {
        addLog('‚ùå Failed to read blob as text, trying as audio...');
        
        // Try to read as ArrayBuffer (for audio)
        try {
          const arrayBuffer = await event.data.arrayBuffer();
          addLog(`üéµ Audio data received: ${arrayBuffer.byteLength} bytes`);
          
          // Play the audio immediately
          await playAudioBuffer(arrayBuffer);
          
          // Update the last message with audio data
          setMessages(prev => {
            const updated = [...prev];
            if (updated.length > 0) {
              const lastMessage = updated[updated.length - 1];
              lastMessage.audioData = arrayBuffer;
              lastMessage.hasAudio = true;
            }
            return updated;
          });
          
        } catch (e) {
          addLog('‚ùå Failed to process as audio data');
        }
      }
    } else {
      addLog(`üì® Received text: ${event.data.substring(0, 100)}...`);
      try {
        const data = JSON.parse(event.data);
        
        // Check for server content and response types
        if (data.serverContent) {
          addLog('‚úÖ Server content received');
          const { modelTurn } = data.serverContent;
          if (modelTurn && modelTurn.parts) {
            const textPart = modelTurn.parts.find((part: any) => part.text);
            const audioPart = modelTurn.parts.find((part: any) => part.inlineData);
            
            if (textPart) {
              addLog(`üí¨ TEXT response: ${textPart.text.substring(0, 50)}...`);
              
              // Update the last message with text response
              setMessages(prev => {
                const updated = [...prev];
                if (updated.length > 0) {
                  const lastMessage = updated[updated.length - 1];
                  lastMessage.response = textPart.text;
                }
                return updated;
              });
            }
            if (audioPart) {
              addLog(`üîä AUDIO response detected! MIME: ${audioPart.inlineData.mimeType}`);
              addLog(`üîç Audio part structure: ${JSON.stringify(Object.keys(audioPart.inlineData))}`);
              
              // Extract and decode base64 audio data
              if (audioPart.inlineData.data) {
                try {
                  const base64Data = audioPart.inlineData.data;
                  addLog(`üìä Base64 data length: ${base64Data.length} characters`);
                  
                  const binaryData = atob(base64Data);
                  const arrayBuffer = new ArrayBuffer(binaryData.length);
                  const uint8Array = new Uint8Array(arrayBuffer);
                  
                  for (let i = 0; i < binaryData.length; i++) {
                    uint8Array[i] = binaryData.charCodeAt(i);
                  }
                  
                  addLog(`üéµ Converting audio data: ${arrayBuffer.byteLength} bytes`);
                  
                  // Collect audio chunks
                  audioChunksRef.current.push(arrayBuffer);
                  addLog(`üì¶ Audio chunks collected: ${audioChunksRef.current.length} total`);
                } catch (error) {
                  addLog(`‚ùå Failed to decode audio data: ${error}`);
                }
              } else {
                addLog(`‚ùå No audio data field found in inlineData`);
                addLog(`üîç Available inlineData fields: ${JSON.stringify(Object.keys(audioPart.inlineData))}`);
              }
            }
          }
          
          // Check for generation complete
          if (data.serverContent && data.serverContent.generationComplete) {
            addLog(`üîÑ Generation complete - processing ${audioChunksRef.current.length} collected audio chunks`);
            await playCollectedAudio();
          }
          
          setIsProcessing(false);
        }
        
        if (data.setupComplete) {
          addLog('‚úÖ Setup complete - ready for audio responses');
        }
        
        // Check for error messages
        if (data.error) {
          addLog(`‚ùå Error: ${data.error.message || data.error}`);
          setIsProcessing(false);
        }
      } catch (e) {
        addLog('‚ùå Failed to parse text message');
      }
    }
  }, [addLog, playAudioBuffer]);

  // EXACT connection logic from debug component
  const connect = useCallback(async () => {
    if (!GEMINI_API_KEY) {
      addLog('‚ùå ERROR: No API key found');
      toast({
        title: "Erro de configura√ß√£o",
        description: "Chave da API n√£o encontrada",
        variant: "destructive",
      });
      return;
    }

    try {
      addLog(`üîå Connecting to Gemini Live API...`);
      
      const ws = new WebSocket(wsUrl);
      websocketRef.current = ws;

      ws.onopen = () => {
        addLog('‚úÖ WebSocket connected successfully');
        setIsConnected(true);
        
        addLog(`üì§ Sending setup message for audio responses...`);
        ws.send(JSON.stringify(setupMessage));
      };

      ws.onmessage = handleMessage;

      ws.onclose = (event) => {
        addLog(`‚ùå Disconnected: ${event.code} - ${event.reason}`);
        setIsConnected(false);
        setIsProcessing(false);
      };

      ws.onerror = (error) => {
        addLog(`‚ùå WebSocket error: ${error}`);
        setIsConnected(false);
        setIsProcessing(false);
      };

    } catch (error) {
      addLog(`‚ùå Connection failed: ${error}`);
      toast({
        title: "Erro de conex√£o",
        description: "Falha ao conectar com Gemini",
        variant: "destructive",
      });
    }
  }, [GEMINI_API_KEY, wsUrl, handleMessage, addLog, toast]);

  // EXACT message sending logic from debug component
  const sendMessage = useCallback(async (text: string) => {
    if (!websocketRef.current || websocketRef.current.readyState !== WebSocket.OPEN) {
      addLog('‚ùå WebSocket not ready');
      toast({
        title: "N√£o conectado",
        description: "Conecte-se primeiro para enviar mensagens",
        variant: "destructive",
      });
      return;
    }

    setIsProcessing(true);
    
    // Clear any previous audio chunks
    audioChunksRef.current = [];
    
    const messageId = Date.now().toString();
    const newMessage: Message = {
      id: messageId,
      userId,
      message: text,
      timestamp: new Date().toISOString()
    };
    
    // Add message to state immediately
    setMessages(prev => [...prev, newMessage]);

    // EXACT message format from debug component
    const messagePayload = {
      clientContent: {
        turns: {
          role: 'user',
          parts: [{ text }]
        },
        turnComplete: true
      }
    };

    addLog(`üì§ Sending message: "${text}"`);
    websocketRef.current.send(JSON.stringify(messagePayload));
  }, [userId, addLog, toast]);

  // Disconnect function
  const disconnect = useCallback(() => {
    if (websocketRef.current) {
      websocketRef.current.close();
      websocketRef.current = null;
    }
    setIsConnected(false);
    setIsProcessing(false);
    addLog('üîå Disconnected from Gemini Live API');
  }, [addLog]);

  // Voice recognition setup (keep from original implementation)
  const startListening = useCallback(() => {
    if (!('webkitSpeechRecognition' in window)) {
      toast({
        title: "N√£o suportado",
        description: "Seu navegador n√£o suporta reconhecimento de voz",
        variant: "destructive",
      });
      return;
    }

    const recognition = new (window as any).webkitSpeechRecognition();
    recognition.lang = 'pt-BR';
    recognition.continuous = false;
    recognition.interimResults = false;

    recognition.onstart = () => {
      addLog('üé§ Voice recognition started');
    };

    recognition.onresult = (event: any) => {
      const transcript = event.results[0][0].transcript;
      addLog(`üé§ Voice transcript: "${transcript}"`);
      sendMessage(transcript);
    };

    recognition.onerror = (event: any) => {
      console.error('Voice recognition error:', event.error);
      toast({
        title: "Erro no reconhecimento de voz",
        description: "Tente novamente",
        variant: "destructive",
      });
    };

    recognition.start();
    return recognition;
  }, [sendMessage, addLog, toast]);

  // Auto-connect on mount
  useEffect(() => {
    connect();
    return () => {
      disconnect();
    };
  }, [connect, disconnect]);

  return {
    messages,
    isConnected,
    isProcessing,
    currentEmotion,
    logs,
    sendMessage,
    startListening,
    connect,
    disconnect
  };
}